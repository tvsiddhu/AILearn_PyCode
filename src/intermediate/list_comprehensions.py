import pandas as pd
import matplotlib.pyplot as plt

# Writing list comprehensions
#
# You now have all the knowledge necessary to begin writing list comprehensions! Your job in this exercise is to write a
# list comprehension that produces a list of the squares of the numbers ranging from 0 to 9.
#

# Create a list of integers: squares
squares = [i ** 2 for i in range(10)]
print(squares)

# Nested list comprehensions
#
# Great! At this point, you have a good grasp of the basic syntax of list comprehensions. Let's push your code-writing
# skills a little further. In this exercise, you will be writing a list comprehension within another list comprehension,
# or nested list comprehensions. It sounds a little tricky, but you can do it!
#
# Let's step aside for a while from strings. One of the ways in which lists can be used are in representing
# multi-dimension objects such as matrices. Matrices can be represented as a list of lists in Python. For example a 5
# x 5 matrix with values 0 to 4 in each row can be written as:
#
# matrix = [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]] Your task is to
# recreate this matrix by using nested listed comprehensions. Recall that you can create one of the rows of the
# matrix with a single list comprehension. To create the list of lists, you simply have to supply the list
# comprehension as the output expression of the overall list comprehension:
#
# [[output expression] for iterator variable in iterable]
# Note that here, the output expression is itself a list comprehension.

# Create a 5 x 5 matrix using a list of lists: matrix
matrix = [[col for col in range(5)] for row in range(5)]

# Print the matrix
for row in matrix:
    print(row)

# Using conditionals in comprehensions

# Create a list of strings: fellowship
fellowship = ['frodo', 'samwise', 'merry', 'pippin', 'aragorn', 'boromir', 'legolas', 'gimli', 'gandalf']

# Create list comprehension: new_fellowship
# new_fellowship = [member for member in fellowship if len(member) >= 7]
new_fellowship = [member if len(member) >= 7 else '' for member in fellowship]

# Print the new list
print("--------------------------------")
print(new_fellowship)

# Dict comprehensions
#

# Create a list of strings: fellowship
fellowship = ['frodo', 'samwise', 'merry', 'pippin', 'aragorn', 'boromir', 'legolas', 'gimli', 'gandalf']

# Create dict comprehension: new_fellowship
# new_fellowship = {member: len(member) for member in fellowship}
new_fellowship = {member: len(member) if len(member) >= 7 else 0 for member in fellowship}

# Print the new dictionary
print("--------------------------------")
print(new_fellowship)

# Write your own generator expressions
#

# Create generator object: result
result = (num for num in range(31))

# Print the first 5 values
print("--------------------------------")
print(next(result))
print(next(result))
print(next(result))
print(next(result))
print(next(result))

# Print the rest of the values
print("--------------------------------")
for value in result:
    print(value)

# Changing the output in generator expressions
#

# Create a list of strings: lannister
lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']

# Create a generator object: lengths
# lengths = (len(person) for person in lannister)
lengths = ((person, len(person)) for person in lannister)

# Iterate over and print the values in lengths
print("--------------------------------")
for value in lengths:
    print(value)

# Build a generator

# Create a list of strings
lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']


# Define generator function get_lengths
def get_lengths(input_list):
    """Generator function that yields the length of the strings in input_list."""
    # Yield the length of a string
    for person in input_list:
        yield len(person)


# Print the values generated by get_lengths()
print("--------------------------------")
for value in get_lengths(lannister):
    print(value)

# List comprehensions for time-stamped data
#

# Extract the created_at column from df: tweet_time


df = pd.read_csv('../../data/learning_python_sources/tweets.csv')
tweet_time = df['created_at']

# Extract the clock time: tweet_clock_time
tweet_clock_time = [entry[11:19] for entry in tweet_time]

# Print the extracted times
print("--------------------------------")
print(tweet_clock_time)

# Conditional list comprehensions for time-stamped data
#

# Extract the created_at column from df: tweet_time
tweet_time = df['created_at']

# Extract the clock time: tweet_clock_time
tweet_clock_time = [entry[11:19] for entry in tweet_time if entry[17:19] == '19']

# Print the extracted times
print("--------------------------------")
print(tweet_clock_time)

# Welcome to the case study!
#

# Zip lists: zipped_lists
df = pd.read_csv('../../data/learning_python_sources/world_ind_pop_data.csv')
feature_names = df.columns
row_vals = df.values
zipped_lists = zip(feature_names, row_vals)

# Create a dictionary: rs_dict
rs_dict = {key: value for key, value in zipped_lists}

# Print the dictionary
print("--------------------------------")
print(rs_dict)


# Define lists2dict()
def lists2dict(list1, list2):
    """Return a dictionary where list1 provides the keys and list2 provides the values."""

    # Zip lists: zipped_lists
    zipped_lists = zip(list1, list2)

    # Create a dictionary: rs_dict
    rs_dict = zip(list1, list2)
    # rs_dict = {key: value for key, value in zipped_lists}

    # Return the dictionary
    return rs_dict


# Call lists2dict: rs_fxn
rs_fxn = lists2dict(feature_names, row_vals)

# Print rs_fxn
print("--------------------------------")
print(rs_fxn)

# Using a list comprehension

# Print the first two lists in row_lists
row_lists = df.values.tolist()
print("--------------------------------")
print(row_lists[0])
print(row_lists[1])

# Turn list of lists into list of dicts: list_of_dicts
list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists]

# Print the first two dictionaries in list_of_dicts
print("--------------------------------")
print(list_of_dicts[0])
print(list_of_dicts[1])

# Turning this all into a DataFrame

# Turn list of lists into list of dicts: list_of_dicts
list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists]

# Turn list of dicts into a DataFrame: df
df = pd.DataFrame(list_of_dicts)

# Print the head of the DataFrame
print("--------------------------------")
print(df.head())

# Processing data in chunks (1)
#

# Open a connection to the file
with open('../../data/learning_python_sources/world_ind_pop_data.csv') as file:
    # Skip the column names
    file.readline()

    # Initialize an empty dictionary: counts_dict
    counts_dict = {}

    # Process only the first 1000 rows
    for j in range(1000):

        # Split the current line into a list: line
        line = file.readline().split(',')

        # Get the value for the first column: first_col
        first_col = line[0]

        # If the column value is in the dict, increment its value
        if first_col in counts_dict.keys():
            counts_dict[first_col] += 1

        # Else, add to the dict and set value to 1
        else:
            counts_dict[first_col] = 1

# Print the resulting dictionary
print("--------------------------------")
print(counts_dict)


# Writing a generator to load data in chunks (2)
#
# In the previous exercise, you processed a file line by line for a given number of lines. What if, however, you want to
# do this for the entire file?
#
# In this case, it would be useful to use generators. Generators allow users to lazily evaluate data. This concept of
# lazy evaluation is useful when you have to deal with very large datasets because it lets you generate values in an
# efficient manner by yielding only chunks of data at a time instead of the whole thing at once.
#
# In this exercise, you will define a generator function read_large_file() that produces a generator object which yields
# a single line from a file each time next() is called on it. The csv file 'world_ind_pop_data.csv' is in your current
# directory for your use.
#
# Note that when you open a connection to a file, the resulting file object is already a generator! So out in the wild,
# you won't have to explicitly create generator objects in cases such as this. However, for pedagogical reasons, we are
# having you practice how to do this here with the read_large_file() function. Go for it!

# Define read_large_file()
def read_large_file(file_object):
    """A generator function to read a large file lazily."""

    # Loop indefinitely until the end of the file
    while True:

        # Read a line from the file: data
        data = file_object.readline()

        # Break if this is the end of the file
        if not data:
            break

        # Yield the line of data
        data = file_object.readline()
        yield data


# Open a connection to the file
with open('../../data/learning_python_sources/world_ind_pop_data.csv') as file:
    # Create a generator object for the file: gen_file
    gen_file = read_large_file(file)

    # Print the first three lines of the file
    print("--------------------------------")
    print(next(gen_file))
    print(next(gen_file))
    print(next(gen_file))

# Writing a generator to load data in chunks (3)
#
# Great! You've just created a generator function that you can use to help you process large files.
#
# Now let's use your generator function to process the World Bank dataset like you did previously. You will process the
# file line by line, to create a dictionary of the counts of how many times each country appears in a column in the
# dataset. For this exercise, however, you won't process just 1000 rows of data, you'll process the entire dataset!
#
# The generator function read_large_file() and the csv file 'world_ind_pop_data.csv' are preloaded and ready for your
# use. Go for it!

# Initialize an empty dictionary: counts_dict
counts_dict = {}

# Open a connection to the file
with open('../../data/learning_python_sources/world_dev_ind.csv') as file:
    # Create a generator object for the file: gen_file
    gen_file = read_large_file(file)

    # Iterate over the generator object
    for line in gen_file:

        # Split the current line into a list: line
        line = line.split(',')

        # Get the value for the first column: first_col
        first_col = row[0]

        # If the column value is in the dict, increment its value
        if first_col in counts_dict.keys():
            counts_dict[first_col] += 1
        else:
            counts_dict[first_col] = 1

# Print the resulting dictionary
print("--------------------------------")
print(counts_dict)

# Writing an iterator to load data in chunks (1)
#
# Another way to read data too large to store in memory in chunks is to read the file in as DataFrames of a certain
# length, say, 100. For example, with the pandas package (imported as pd), you can do pd.read_csv(filename,
# chunksize=100).
#
# This creates an iterable reader object, which means that you can use next() on it.
#
# In this exercise, you will read a file in small DataFrame chunks with read_csv(). You're going to use the World Bank
# Indicators data 'ind_pop.csv', available in your current directory, to look at the urban population indicator for
# numerous countries and years.

# Initialize reader object: df_reader
df_reader = pd.read_csv('../../data/learning_python_sources/world_ind_pop_data.csv', chunksize=10)

# Print two chunks
print("--------------------------------")
print(next(df_reader))
print(next(df_reader))

# Writing an iterator to load data in chunks (2)
#
# In the previous exercise, you used read_csv() to read in DataFrame chunks from a large dataset. In this exercise,
# you will read in a file using a bigger DataFrame chunk size and then process the data from the first chunk.
#
# To process the data, you will create another DataFrame composed of only the rows from a specific country. You will
# then zip together two of the columns from the new DataFrame, 'Total Population' and 'Urban population (% of total)'.
#
# You're going to use the data from 'ind_pop_data.csv', available in your current directory. Pandas has been imported
# as pd.

# Initialize reader object: urb_pop_reader
urb_pop_reader = pd.read_csv('../../data/learning_python_sources/world_ind_pop_data.csv', chunksize=1000)

# Get the first DataFrame chunk: df_urb_pop
df_urb_pop = next(urb_pop_reader)

# Check out the head of the DataFrame
print("--------------------------------")
print(df_urb_pop.head())

# Check out specific country: df_pop_ceb
df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']

# Zip DataFrame columns of interest: pops
pops = zip(df_pop_ceb['Total Population'], df_pop_ceb['Urban population (% of total)'])

# Turn zip object into a list: pops_list
pops_list = list(pops)

# Print pops_list
print("--------------------------------")
print(pops_list)

# Writing an iterator to load data in chunks (3)
#
# You're getting used to reading and processing data in chunks by now. Let's push your skills a little further by
# adding a column to a DataFrame.
#
# Starting from the code of the previous exercise, you will be using a list comprehension to create the values for a new
# column 'Total Urban Population' from the list of tuples that you generated earlier. Recall from the previous exercise
# that the first and second elements of each tuple consist of, respectively, values from the columns 'Total Population'
# and 'Urban population (% of total)'. The values in this new column 'Total Urban Population', therefore, are the
# product of the first and second element in each tuple. Furthermore, because the 2nd element is a percentage, you need
# to divide the entire result by 100, or alternatively, multiply it by 0.01.
#
# You will also plot the data from this new column to create a visualization of the urban population data.


# Code from previous exercise
urb_pop_reader = pd.read_csv('../../data/learning_python_sources/world_ind_pop_data.csv', chunksize=1000)
df_urb_pop = next(urb_pop_reader)
df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']
pops = zip(df_pop_ceb['Total Population'], df_pop_ceb['Urban population (% of total)'])
pops_list = list(pops)

# Use list comprehension to create new DataFrame column 'Total Urban Population'
df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]

# Plot urban population data
df_pop_ceb.plot(kind='scatter', x='Year', y='Total Urban Population')
plt.show()

# Writing an iterator to load data in chunks (4)
#
# In the previous exercises, you've only processed the data from the first DataFrame chunk. This time,
# you will aggregate the results over all the DataFrame chunks in the dataset. This basically means you will be
# processing the entire dataset now. This is neat because you're going to be able to process the entire large dataset
# by just working on smaller pieces of it!

# Initialize reader object: urb_pop_reader
urb_pop_reader = pd.read_csv('../../data/learning_python_sources/world_ind_pop_data.csv', chunksize=1000)

# Initialize empty DataFrame: data
data = pd.DataFrame()

# Iterate over each DataFrame chunk
for df_urb_pop in urb_pop_reader:
    # Check out specific country: df_pop_ceb
    df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']

    # Zip DataFrame columns of interest: pops
    pops = zip(df_pop_ceb['Total Population'], df_pop_ceb['Urban population (% of total)'])

    # Turn zip object into list: pops_list
    pops_list = list(pops)

    # Use list comprehension to create new DataFrame column 'Total Urban Population'
    df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]

    # Append DataFrame chunk to data: data
    data = pd.concat([data, df_pop_ceb])

# Plot urban population data
data.plot(kind='scatter', x='Year', y='Total Urban Population')
plt.show()


# Writing an iterator to load data in chunks (5)
#
# This is the last leg. You've learned a lot about processing a large dataset in chunks. In this last exercise, you will
# put all the code for processing the data into a single function so that you can reuse the code without having to
# rewrite the same things all over again.
#
# You're going to define the function plot_pop() which takes two arguments: the filename of the file to be processed,
# and the country code of the rows you want to process in the dataset.
#
# Because all of the previous code you've written in the previous exercises will be housed in plot_pop(), calling the
# function already does the following:
#
# Loading of the file chunk by chunk,
# Creating the new column of urban population values, and
# Plotting the urban population data.
# That's a lot of work, but the function now makes it convenient to repeat the same process for whatever file and
# country code you want to process and visualize!

# Define plot_pop()
def plot_pop(filename, country_code):
    """Function to plot urban population data."""

    # Initialize reader object: urb_pop_reader
    urb_pop_reader = pd.read_csv(filename, chunksize=1000)

    # Initialize empty DataFrame: data
    data = pd.DataFrame()

    # Iterate over each DataFrame chunk
    for df_urb_pop in urb_pop_reader:
        # Check out specific country: df_pop_ceb
        df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == country_code]

        # Zip DataFrame columns of interest: pops
        pops = zip(df_pop_ceb['Total Population'], df_pop_ceb['Urban population (% of total)'])

        # Turn zip object into list: pops_list
        pops_list = list(pops)

        # Use list comprehension to create new DataFrame column 'Total Urban Population'
        df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]

        # Append DataFrame chunk to data: data
        data = pd.concat([data, df_pop_ceb])

    # Plot urban population data
    data.plot(kind='scatter', x='Year', y='Total Urban Population')
    plt.show()


# Call plot_pop for country code 'CEB'
plot_pop('../../data/learning_python_sources/world_ind_pop_data.csv', 'CEB')

# Call plot_pop for country code 'ARB'
plot_pop('../../data/learning_python_sources/world_ind_pop_data.csv', 'ARB')
